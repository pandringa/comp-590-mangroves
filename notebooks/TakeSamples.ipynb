{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TakeSamples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is used to collect LANDSAT images and sample them from specified mangrove regions, uploading `.tfrecord` files to Google Cloud in preparation for a machine learning model to work with them.\n",
        "\n",
        "It's based on part of a notebook prepared by Google as part of the EarthEngine documentation. You can find links to that original notebook below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP",
        "colab_type": "text"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Authenticate and import as necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIa46CpciXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jat01FEoUMqg",
        "colab_type": "code",
        "outputId": "643b4889-e290-4a81-cf82-0fca7db68cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=G-ZqCSSE2kk411FzMV3NLWIqm57pC7Kujy8EOql4I3Y&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/zAEoKdRPHlyTzvx1Eojj7mTT4l3jO4CYimKVPw0DotG6nL1rFwn1XeA\n",
            "\n",
            "Successfully saved authorization token.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RnZzcYhcpsQ",
        "colab_type": "code",
        "outputId": "4f8d8388-c24e-4f09-e0f4-04687f5f4c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Tensorflow setup.\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.enable_eager_execution() ## enabled by default\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1hFdpBQfyhN",
        "colab_type": "code",
        "outputId": "85ed8f81-2640-4491-e04e-f85158ca9175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8ycmzClYwf",
        "colab_type": "text"
      },
      "source": [
        "# Variables\n",
        "\n",
        "Declare the variables that will be in use throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKs6HuxOzjMl",
        "colab_type": "text"
      },
      "source": [
        "## Specify your Cloud Storage Bucket\n",
        "You must have write access to a bucket to run this demo!  To run it read-only, use the demo bucket below, but note that writes to this bucket will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obDDH1eDzsch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INSERT YOUR BUCKET HERE:\n",
        "BUCKET = 'mangroves_model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ",
        "colab_type": "text"
      },
      "source": [
        "## Set other global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psz7wJKalaoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'fcnn-caribbean-mangroves/data-model-3'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = [] ## Not on Lansat 7\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'constant'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "RESOLUTION = 30\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']\n",
        "\n",
        "YEAR = \"2016\"\n",
        "\n",
        "TEST_LOC = [8.527453749657104,-83.29948528935525]\n",
        "CARIBBEAN_GEO = ee.Geometry.Polygon(\n",
        "         [[-117, 32],\n",
        "          [-117, 0],\n",
        "          [-53, 0],\n",
        "          [-53, 32]]);\n",
        "\n",
        "TEST_GEO_SMALL = ee.Geometry.Polygon(\n",
        "        [[[-83.86332831505052, 9.267295185967846],\n",
        "          [-83.86332831505052, 8.03455825553625],\n",
        "          [-82.17692694786302, 8.03455825553625],\n",
        "          [-82.17692694786302, 9.267295185967846]]]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4",
        "colab_type": "text"
      },
      "source": [
        "# Imagery\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a one-year, cloud-free, Landsat 7 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IlgXu-vcUEY",
        "colab_type": "code",
        "outputId": "7a229e3b-737b-436b-c21a-733de5ec7385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "print(\"Year is \"+str(YEAR))\n",
        "image = l8sr.filterDate(YEAR+'-01-01', YEAR+'-12-31').map(maskL8sr).median()\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B3', 'B2', 'B1'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=TEST_LOC)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Year is 2016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"about:blank\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" data-html=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 onload=\"this.contentDocument.open();this.contentDocument.write(atob(this.getAttribute('data-html')));this.contentDocument.close();\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f2342bd8dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ",
        "colab_type": "text"
      },
      "source": [
        "Prepare the response (what we want to predict).  In this case, we're looking at mangroves. We have to convert the vector FeatureCollection into an Image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0wHDyxVirec",
        "colab_type": "code",
        "outputId": "f43117db-7609-4e23-f799-8502e538f0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "mangrove_shapes = ee.FeatureCollection(\"users/pandringa/gmw_\"+YEAR)\n",
        "\n",
        "train_geo = ee.FeatureCollection(\"users/andreagonzales/caribbean_train_areas_\"+YEAR);\n",
        "test_geo = ee.FeatureCollection(\"users/andreagonzales/caribbean_test_areas_\"+YEAR);\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(train_geo, 1).paint(test_geo, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "print(\"YEAR is \"+YEAR)\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
        "map = folium.Map(location=TEST_LOC, zoom_start=8)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "\n",
        "mangrove_image = ee.Image(0).byte().paint(mangrove_shapes, 1)\n",
        "mapid = mangrove_image.updateMask(mangrove_image).getMapId({'min': 0, 'max': 1, 'palette': ['white', 'black']})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YEAR is 2016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"about:blank\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" data-html=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 onload=\"this.contentDocument.open();this.contentDocument.write(atob(this.getAttribute('data-html')));this.contentDocument.close();\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f23077dc278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg",
        "colab_type": "text"
      },
      "source": [
        "Stack the 2D images (Landsat composite and NLCD impervious surface) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGHYsdAOipa4",
        "colab_type": "code",
        "outputId": "a2af1fab-d2a0-449a-8890-517057ac7177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  mangrove_image.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)\n",
        "\n",
        "print(f\"Year: {YEAR}\")\n",
        "print(f\"Bands: {image.bandNames().getInfo()}\")\n",
        "print(f\"Properties: {image.propertyNames().getInfo()}\")\n",
        "print(f\"Image projection: {image.projection().getInfo()}\")\n",
        "print(f\"Image resolution: {image.projection().nominalScale().getInfo()}\")\n",
        "\n",
        "sample = arrays.sample(\n",
        "  region = test_geo.first().geometry(), \n",
        "  scale = RESOLUTION, \n",
        "  numPixels = 32,\n",
        "  tileScale = 4,\n",
        "  seed = 2\n",
        ")\n",
        "print(f\"Sample: {sample.size().getInfo()}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Year: 2016\n",
            "Bands: ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
            "Properties: ['system:bands', 'system:band_names']\n",
            "Image projection: {'type': 'Projection', 'crs': 'EPSG:4326', 'transform': [1, 0, 0, 0, 1, 0]}\n",
            "Image resolution: 111319.49079327357\n",
            "Sample: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz",
        "colab_type": "text"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "Before we can sample the data, we must do a bunch of pre-processing to prepare it. Specifically, we need to estimate the number of samples we want to take from each polygon, based on the area of the polygon versus the total area of all mangrove polygons. (It uses a square root scale function so that it isn't skewed towards the very largest mangrove forests.)\n",
        "\n",
        "Then, we must also duplicate the polygons that require more than `max_sample` samples, since EarthEngine tends to run into memory issues above that number. By duplicating them and re-setting their sample counts, we effectively sample the correct number from the polygon by visiting it twice (or more) and recording `max_sample` each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyRpvwENxE-A",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "3187f90b-4eb5-4e07-9c35-e146aad22758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "# Estimated sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 14000\n",
        "EVAL_SIZE = 6000\n",
        "\n",
        "# Maximum number of samples at once (to stay under memory limit)\n",
        "max_sample = 32\n",
        "\n",
        "\n",
        "def setSampleCount(areas, target):\n",
        "  sumRoots = ee.List(areas).map(lambda a: ee.Number(a).sqrt()).reduce(ee.Reducer.sum())\n",
        "  def mapFn(f):\n",
        "    f = ee.Feature(f)\n",
        "    return f.set({\n",
        "      ## Log-based curve to weight small areas more than large ones\n",
        "      'samples': ee.Number(f.get('AREA')).sqrt().multiply(target).divide(sumRoots).ceil(),\n",
        "      'seed': 0\n",
        "    })\n",
        "  return mapFn\n",
        "\n",
        "def repeatPoly(f, samples=None, seed=1):\n",
        "  f = ee.Feature(f)\n",
        "\n",
        "  samples = ee.Number(ee.Algorithms.If(samples, samples, f.get('samples')))\n",
        "  needs_repeat = samples.subtract(max_sample);\n",
        "  full_repeats = needs_repeat.divide(max_sample).floor();\n",
        "\n",
        "  f = f.set({ \"samples\": max_sample, \"seed\": None })\n",
        "  coll = ee.List.repeat(f, full_repeats)\n",
        "\n",
        "  f = f.set({ \"samples\": needs_repeat.subtract( full_repeats.multiply(max_sample) ) })\n",
        "  coll = coll.add(f)\n",
        "\n",
        "  features = ee.FeatureCollection(coll)\n",
        "  features = features.randomColumn(\"seed\")\n",
        "  features = features.map(lambda f: f.set({ \"seed\": ee.Number(f.get(\"seed\")).multiply(1000).int() }))\n",
        "\n",
        "  return features\n",
        "\n",
        "def load_and_convert(features_address, samples=10000, name=\"\"):\n",
        "  print(f\"\\n[{name}] Processing FeatureCollection...\")\n",
        "  polys = ee.FeatureCollection(features_address)\n",
        "  count = polys.size().getInfo()\n",
        "  print(f\"[{name}] training count: {count} polys\")\n",
        "\n",
        "  areas = polys.aggregate_array('AREA')\n",
        "  polys = polys.map( setSampleCount(areas, samples) )\n",
        "\n",
        "  # Build list of oversized polys, duplicate them into collection\n",
        "  tooLarge = polys.filter(ee.Filter.gt(\"samples\", max_sample))\n",
        "  print(f\"[{name}] Oversized samples: { tooLarge.size().getInfo() } polys\")\n",
        "  # print(tooLarge.aggregate_array(\"samples\").getInfo())\n",
        "\n",
        "  repeated = tooLarge.map( repeatPoly ).flatten()\n",
        "  repeatSize = repeated.size().getInfo()\n",
        "  print(f\"[{name}] New polys added to list: { repeatSize }\")\n",
        "  # print(repeated.aggregate_array(\"samples\").getInfo())\n",
        "  # print(repeated.aggregate_array(\"seed\").getInfo())\n",
        "\n",
        "  polys = polys.merge(repeated)\n",
        "\n",
        "  polys = polys.map(lambda f: f.set({ 'samples': ee.Algorithms.If(ee.Number(f.get('samples')).gt(max_sample), max_sample, f.get('samples')) }))\n",
        "\n",
        "  # Final counts\n",
        "  count = polys.size().getInfo()\n",
        "  polyList = polys.toList(count)\n",
        "\n",
        "  # Shuffle big ones at the end so they are spread throughout\n",
        "  for i in range( repeatSize ):\n",
        "   newPos = random.randint(0, count-repeatSize-1)\n",
        "   polyList = polyList.swap(count-i-1, newPos)\n",
        "\n",
        "  totalSamples = polys.reduceColumns(ee.Reducer.sum(), [\"samples\"]).get('sum').getInfo()\n",
        "  maxSamples = polys.reduceColumns(ee.Reducer.max(), [\"samples\"]).get('max').getInfo()\n",
        "  print(f\"[{name}] Adjusted count: { count } polys\")\n",
        "  print(f\"[{name}] Estimated samples: { totalSamples }\")\n",
        "  print(f\"[{name}] Largest sample: { maxSamples } \")\n",
        "\n",
        "  print(f\"[{name}] Overall average sample per poly: {totalSamples / count }\")\n",
        "  endAvg = polyList.slice(count-repeatSize-1, count).map(lambda f: ee.Feature(f).get(\"samples\")).reduce(ee.Reducer.mean()).getInfo()\n",
        "  print(f\"[{name}] Average sample of last {repeatSize} polys: {endAvg}\")\n",
        "\n",
        "  # Export to EE for debug\n",
        "  # asset_name = f\"{name}_sample_geos_sqrt\"\n",
        "  # print(f\"[{name}] Exporting feature collection to EarthEngine asset users/pandringa/{asset_name}\")\n",
        "  # ee.batch.Export.table.toAsset(\n",
        "  #     collection=polys,\n",
        "  #     description=asset_name,\n",
        "  #     assetId=\"users/pandringa/\"+asset_name\n",
        "  # ).start()\n",
        "\n",
        "  return polyList, count, totalSamples\n",
        "\n",
        "print(\"YEAR is \"+YEAR)\n",
        "trainPolysList, trainCount, trainSampleCount = (load_and_convert(\n",
        "    \"users/andreagonzales/caribbean_train_areas_\"+YEAR,\n",
        "    samples=TRAIN_SIZE,\n",
        "    name=\"TRAIN\"\n",
        "))\n",
        "\n",
        "evalPolysList, evalCount, evalSampleCount = (load_and_convert(\n",
        "    \"users/andreagonzales/caribbean_test_areas_\"+YEAR,\n",
        "    samples=EVAL_SIZE,\n",
        "    name=\"EVAL\"\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YEAR is 2016\n",
            "\n",
            "[TRAIN] Processing FeatureCollection...\n",
            "[TRAIN] training count: 1278 polys\n",
            "[TRAIN] Oversized samples: 65 polys\n",
            "[TRAIN] New polys added to list: 92\n",
            "[TRAIN] Adjusted count: 1370 polys\n",
            "[TRAIN] Estimated samples: 14627\n",
            "[TRAIN] Largest sample: 32 \n",
            "[TRAIN] Overall average sample per poly: 10.676642335766424\n",
            "[TRAIN] Average sample of last 92 polys: 10.344086021505376\n",
            "\n",
            "[EVAL] Processing FeatureCollection...\n",
            "[EVAL] training count: 536 polys\n",
            "[EVAL] Oversized samples: 25 polys\n",
            "[EVAL] New polys added to list: 38\n",
            "[EVAL] Adjusted count: 574 polys\n",
            "[EVAL] Estimated samples: 6279\n",
            "[EVAL] Largest sample: 32 \n",
            "[EVAL] Overall average sample per poly: 10.939024390243903\n",
            "[EVAL] Average sample of last 38 polys: 11.564102564102564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9QubZ0XgwtV",
        "colab_type": "text"
      },
      "source": [
        "Now, we take samples from each polygon and merge the results into a single export. The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point. It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record. You do NOT need to export each training/testing patch to a different image. Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error. Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export.\n",
        "\n",
        "Finally, it also uses a little recursion trick to get around the `Maximum depth exceeded` error from EarthEngine which occurred when building up a FeatureCollection using `.merge` in a simple loop. By taking a recursive, split-down-the-middle approach, we reduce the depth of the command issued to EarthEngine from `N` to `log(N)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRRp-F6Wgu6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "# These numbers determined experimentally.\n",
        "tileScale = 8\n",
        "shard_size = 1000 # 1000 samples = 400mb\n",
        "\n",
        "def merge_samples(geos, start, end):\n",
        "  if end == start:\n",
        "    geo = ee.Feature(geos.get(start))\n",
        "    samples = ee.Number(geo.get('samples'))\n",
        "    return arrays.sample(\n",
        "      region = geo.geometry(),\n",
        "      scale = RESOLUTION,\n",
        "      numPixels = ee.Algorithms.If(samples.lte(max_sample), samples, max_sample),\n",
        "      tileScale = tileScale,\n",
        "      seed = geo.get('seed')\n",
        "    )\n",
        "  else:\n",
        "    pivot = math.floor((end - start) / 2) + start\n",
        "    return merge_samples(geos, start, pivot).merge( merge_samples(geos, pivot+1, end) )\n",
        "\n",
        "## Sample usage for visual checking\n",
        "# ee.batch.Export.table.toAsset(\n",
        "#     collection = merge_samples(trainPolysList, 0, trainCount-1 ),\n",
        "#     description = \"test_training_sample_full\",\n",
        "#     assetId = \"users/pandringa/test_train_area_5th_sample_\"+YEAR,\n",
        "# ).start()\n",
        "\n",
        "# ee.batch.Export.table.toAsset(\n",
        "#     collection = merge_samples(evalPolysList, 0, evalCount-1 ),\n",
        "#     description = \"test_eval_sample_full\",\n",
        "#     assetId = \"users/pandringa/test_eval_area_5th_sample_\"+YEAR,\n",
        "# ).start()\n",
        "\n",
        "## Build training data\n",
        "shards = math.ceil(trainSampleCount / shard_size)\n",
        "shard_polys = math.ceil(trainCount / shards)\n",
        "print(f\"\\nSampling {trainSampleCount} eval samples, in {shards} shards...\")\n",
        "for i in range( shards ):\n",
        "  shard_start = i * shard_polys\n",
        "  shard_end = min(shard_start+shard_polys, trainCount)\n",
        "  geomSample = merge_samples(\n",
        "      trainPolysList, \n",
        "      shard_start, \n",
        "      shard_end-1\n",
        "  )\n",
        "\n",
        "  desc = TRAINING_BASE + '_' + YEAR +'_s' + str(i)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc, \n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "  print(f\"Started Google Cloud task for {YEAR} training data shard {i}.\");\n",
        "\n",
        "## Build eval data\n",
        "shards = math.ceil(evalSampleCount / shard_size)\n",
        "shard_polys = math.ceil(evalCount / shards)\n",
        "print(f\"\\nSampling {evalSampleCount} eval samples, in {shards} shards...\")\n",
        "for i in range( shards ):\n",
        "  shard_start = i * shard_polys\n",
        "  shard_end = min(shard_start+shard_polys, evalCount)\n",
        "  geomSample = merge_samples(\n",
        "      evalPolysList,\n",
        "      shard_start,\n",
        "      shard_end-1\n",
        "  )\n",
        "\n",
        "  desc = EVAL_BASE + '_' + YEAR +'_s' + str(i)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc, \n",
        "    bucket = BUCKET, \n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "  print(f\"Started Google Cloud task for {YEAR} eval data shard {i}.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6eVSgFI20E",
        "colab_type": "text"
      },
      "source": [
        "Now you're done! Check the [Earth Engine Console](https://code.earthengine.google.com) to view the status of all your tasks, and once they're done you can move onto the other notebooks that handle data loading and model training."
      ]
    }
  ]
}